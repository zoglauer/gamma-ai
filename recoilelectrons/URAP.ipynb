{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QrvsS-hrpKvA",
    "outputId": "94a23683-0d0a-4c63-d7ab-ee66a2cb6fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compton Track Identification\n",
      "============================\n",
      "\n",
      "Results_20211129_052151\n",
      "\n",
      "\n",
      "Started reading data sets\n",
      "Info: Parsed 10000 events\n",
      "Info: Number of training data sets: 9088   Number of testing data sets: 896 (vs. input: 10000 and split ratio: 0.1)\n",
      "Info: Setting up neural network...\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_37 (Conv3D)           (None, 30, 30, 62, 128)   3584      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 15, 15, 20, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 13, 13, 18, 64)    221248    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 6, 6, 9, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 4, 4, 7, 32)       55328     \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 3584)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                229440    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 509,990\n",
      "Trainable params: 509,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Info: Training and evaluating the network\n",
      "\n",
      "\n",
      "Starting iteration 1\n",
      "Batch 1 / 71\n",
      "4/4 [==============================] - 98s 24s/step - loss: 196153856.0000 - mse: 196153856.0000 - val_loss: inf - val_mse: inf\n",
      "Batch 2 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 3 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 4 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 5 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 6 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 7 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 8 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 9 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 10 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 11 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 12 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 13 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 14 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 15 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 16 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 17 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 18 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 19 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 20 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 21 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 22 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 23 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 24 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 25 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 26 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 27 / 71\n",
      "4/4 [==============================] - 119s 32s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 28 / 71\n",
      "4/4 [==============================] - 98s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 29 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 30 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 31 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 32 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 33 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 34 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 35 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 36 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 37 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 38 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 39 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 40 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 41 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 42 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 43 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 44 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 45 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 46 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 47 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 48 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 49 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 50 / 71\n",
      "4/4 [==============================] - 83s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 51 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 52 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 53 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 54 / 71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 55 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 56 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 57 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 58 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 59 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 60 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 61 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 62 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 63 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 64 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 65 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 66 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 67 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 68 / 71\n",
      "4/4 [==============================] - 82s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 69 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 70 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Batch 71 / 71\n",
      "4/4 [==============================] - 81s 20s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "\n",
      "Current loss: nan\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d7ece29849a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    430\u001b[0m   \u001b[0mTimerTesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nCurrent loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m   \u001b[0mImprovement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDist_Diff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAngle_Diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckPerformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mImprovement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d7ece29849a5>\u001b[0m in \u001b[0;36mCheckPerformance\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mImprovement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSumDistDiff\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mTotalEvents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSumAngleDiff\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mTotalEvents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import signal\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "\n",
    "print(\"\\nCompton Track Identification\")\n",
    "print(\"============================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Input parameters\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Default parameters\n",
    "\n",
    "# X, Y, Z bins\n",
    "XBins = 32\n",
    "YBins = 32\n",
    "ZBins = 64\n",
    "\n",
    "# File names\n",
    "FileName = \"RecoilElectrons.inc1.id1.data\"\n",
    "\n",
    "# MY CODE\n",
    "FileName = \"RecoilElectrons.10k.data\"\n",
    "# MY CODE\n",
    "\n",
    "# Depends on GPU memory and layout\n",
    "BatchSize = 128\n",
    "\n",
    "# Split between training and testing data\n",
    "TestingTrainingSplit = 0.1\n",
    "\n",
    "MaxEvents = 100000\n",
    "\n",
    "\n",
    "# Determine derived parameters\n",
    "\n",
    "OutputDataSpaceSize = 6\n",
    "\n",
    "XMin = -43\n",
    "XMax = 43\n",
    "\n",
    "# XMin = -5\n",
    "# XMax = +5\n",
    "\n",
    "YMin = -43\n",
    "YMax = 43\n",
    "\n",
    "# YMin = -5\n",
    "# YMax = +5\n",
    "\n",
    "ZMin = 13\n",
    "ZMax = 45\n",
    "\n",
    "OutputDirectory = \"Results\"\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Perform training and/or testing of the event clustering machine learning tools.')\n",
    "# parser.add_argument('-f', '--filename', default='ComptonTrackIdentification.p1.sim.gz', help='File name used for training/testing')\n",
    "# parser.add_argument('-m', '--maxevents', default='10000', help='Maximum number of events to use')\n",
    "# parser.add_argument('-s', '--testingtrainingsplit', default='0.1', help='Testing-training split')\n",
    "# parser.add_argument('-b', '--batchsize', default='128', help='Batch size')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# print(args)\n",
    "\n",
    "# if args.filename != \"\":\n",
    "#   FileName = args.filename\n",
    "\n",
    "# if int(args.maxevents) > 1000:\n",
    "#   MaxEvents = int(args.maxevents)\n",
    "\n",
    "# if int(args.batchsize) >= 16:\n",
    "#   BatchSize = int(args.batchsize)\n",
    "\n",
    "# if float(args.testingtrainingsplit) >= 0.05:\n",
    "#    TestingTrainingSplit = float(args.testingtrainingsplit)\n",
    "\n",
    "if not os.path.exists(OutputDirectory):\n",
    "  os.makedirs(OutputDirectory)\n",
    "Now = datetime.now()\n",
    "OutputDirectory += Now.strftime(\"_%Y%m%d_%H%M%S\")\n",
    "print(OutputDirectory)\n",
    "\n",
    "###################################################################################################\n",
    "# Step 2: Global functions\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Take care of Ctrl-C\n",
    "Interrupted = False\n",
    "NInterrupts = 0\n",
    "def signal_handler(signal, frame):\n",
    "  global Interrupted\n",
    "  Interrupted = True\n",
    "  global NInterrupts\n",
    "  NInterrupts += 1\n",
    "  if NInterrupts >= 2:\n",
    "    print(\"Aborting!\")\n",
    "    sys.exit(0)\n",
    "  print(\"You pressed Ctrl+C - waiting for graceful abort, or press Ctrl-C again, for quick exit.\")\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "\n",
    "# Everything ROOT related can only be loaded here otherwise it interferes with the argparse\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive')\n",
    "\n",
    "from EventData import EventData\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Step 3: Read the data\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "print(\"\\n\\nStarted reading data sets\")\n",
    "\n",
    "with open(FileName, \"rb\") as FileHandle:\n",
    "\n",
    "  DataSets = pickle.load(FileHandle)\n",
    "\n",
    "NumberOfDataSets = len(DataSets)\n",
    "\n",
    "print(\"Info: Parsed {} events\".format(NumberOfDataSets))\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Step 4: Split the data into training, test & verification data sets\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Split the data sets in training and testing data sets\n",
    "\n",
    "# The number of available batches in the inoput data\n",
    "NBatches = int(len(DataSets) / BatchSize)\n",
    "if NBatches < 2:\n",
    "  print(\"Not enough data!\")\n",
    "  quit()\n",
    "\n",
    "# Split the batches in training and testing according to TestingTrainingSplit\n",
    "NTestingBatches = int(NBatches*TestingTrainingSplit)\n",
    "if NTestingBatches == 0:\n",
    "  NTestingBatches = 1\n",
    "NTrainingBatches = NBatches - NTestingBatches\n",
    "\n",
    "# Now split the actual data:\n",
    "TrainingDataSets = []\n",
    "for i in range(0, NTrainingBatches * BatchSize):\n",
    "  TrainingDataSets.append(DataSets[i])\n",
    "\n",
    "\n",
    "TestingDataSets = []\n",
    "for i in range(0,NTestingBatches*BatchSize):\n",
    "   TestingDataSets.append(DataSets[NTrainingBatches * BatchSize + i])\n",
    "\n",
    "\n",
    "NumberOfTrainingEvents = len(TrainingDataSets)\n",
    "NumberOfTestingEvents = len(TestingDataSets)\n",
    "\n",
    "print(\"Info: Number of training data sets: {}   Number of testing data sets: {} (vs. input: {} and split ratio: {})\".format(NumberOfTrainingEvents, NumberOfTestingEvents, len(DataSets), TestingTrainingSplit))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Step 5: Setting up the neural network\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "print(\"Info: Setting up neural network...\")\n",
    "\n",
    "  \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6144)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "Model = models.Sequential()\n",
    "Model.add(layers.Conv3D(128, (3, 3, 3), activation='relu', input_shape=(XBins, YBins, ZBins, 1)))\n",
    "Model.add(layers.MaxPooling3D((2, 2, 3)))\n",
    "Model.add(layers.Conv3D(64, (3, 3, 3), activation='relu'))\n",
    "Model.add(layers.MaxPooling3D((2, 2, 2)))\n",
    "Model.add(layers.Conv3D(32, (3, 3, 3), activation='relu'))\n",
    "Model.add(layers.Flatten())\n",
    "Model.add(layers.Dense(64, activation='relu'))\n",
    "Model.add(layers.Dense(OutputDataSpaceSize))\n",
    "\n",
    "\n",
    "#Model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "Model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "    \n",
    "Model.summary()\n",
    "\n",
    "#K FOLD\n",
    "\n",
    "def createModel():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "  Model.add(layers.BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "  Model.add(layers.BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(no_classes, activation='softmax'))\n",
    "  return model\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Step 6: Training and evaluating the network\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "print(\"Info: Training and evaluating the network\")\n",
    "\n",
    "# Train the network\n",
    "BestLoss = sys.float_info.max\n",
    "IterationOutputInterval = 10\n",
    "CheckPointNum = 0\n",
    "\n",
    "\n",
    "BestLocation = 100000\n",
    "BestAngle = 180\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "def CheckPerformance():\n",
    "  global BestLocation\n",
    "  global BestAngle\n",
    "\n",
    "  Improvement = False\n",
    "\n",
    "  TotalEvents = 0\n",
    "  SumDistDiff = 0\n",
    "  SumAngleDiff = 0\n",
    "\n",
    "  # Step run all the testing batches, and detrmine the percentage of correct identifications\n",
    "  # Step 1: Loop over all Testing batches\n",
    "  for Batch in range(0, NTestingBatches):\n",
    "\n",
    "    # Step 1.1: Convert the data set into the input and output tensor\n",
    "    InputTensor = np.zeros(shape=(BatchSize, XBins, YBins, ZBins, 1))\n",
    "    OutputTensor = np.zeros(shape=(BatchSize, OutputDataSpaceSize))\n",
    "\n",
    "\n",
    "    # Loop over all testing  data sets and add them to the tensor\n",
    "    for e in range(0, BatchSize):\n",
    "      Event = TestingDataSets[e + Batch*BatchSize]\n",
    "      # Set the layer in which the event happened\n",
    "\n",
    "      # Set all the hit locations and energies\n",
    "      for h in range(0, len(Event.X)):\n",
    "        XBin = int( (Event.X[h] - XMin) / ((XMax - XMin) / XBins) )\n",
    "        YBin = int( (Event.Y[h] - YMin) / ((YMax - YMin) / YBins) )\n",
    "        ZBin = int( (Event.Z[h] - ZMin) / ((ZMax - ZMin) / ZBins) )\n",
    "        #print(\"hit z bin: {} {}\".format(Event.Z[h], ZBin))\n",
    "        if XBin >= 0 and YBin >= 0 and ZBin >= 0 and XBin < XBins and YBin < YBins and ZBin < ZBins:\n",
    "          InputTensor[e][XBin][YBin][ZBin][0] = Event.E[h]\n",
    "\n",
    "\n",
    "\n",
    "    # Step 2: Run it\n",
    "    # Result = Session.run(Output, feed_dict={X: InputTensor})\n",
    "    Result = Model.predict(InputTensor)\n",
    "\n",
    "    #print(Result[e])\n",
    "    #print(OutputTensor[e])\n",
    "    DistDiffList, AngleDiffList = [], []\n",
    "\n",
    "    for e in range(0, BatchSize):\n",
    "      Event = TestingDataSets[e + Batch*BatchSize]\n",
    "      \n",
    "      oPos = np.array([ Event.TrackStartX, Event.TrackStartY, Event.TrackStartZ ])\n",
    "      rPos = np.array([ Result[e][0], Result[e][1], Result[e][2] ])\n",
    "      \n",
    "      oDir = np.array([ Event.TrackDirectionX, Event.TrackDirectionY, Event.TrackDirectionZ ])\n",
    "      rDir = np.array([ Result[e][3], Result[e][4], Result[e][5] ])\n",
    "      \n",
    "      # Distance difference location:\n",
    "      DistDiff = np.linalg.norm(oPos - rPos)\n",
    "      \n",
    "      # Angle difference direction\n",
    "      Norm = np.linalg.norm(oDir)\n",
    "      if Norm == 0:\n",
    "        print(\"Warning: original direction is zero: {} {} {}\".format(Event.DirectionStartX, Event.DirectionStartY, Event.DirectionStartZ))\n",
    "        continue\n",
    "      oDir /= Norm\n",
    "      Norm = np.linalg.norm(rDir)\n",
    "      if Norm == 0:\n",
    "        print(\"Warning: estimated direction is zero: {} {} {}\".format(Result[e][3], Result[e][4], Result[e][5]))\n",
    "        continue\n",
    "      rDir /= Norm\n",
    "      AngleDiff = np.arccos(np.clip(np.dot(oDir, rDir), -1.0, 1.0)) * 180/math.pi\n",
    "\n",
    "      if math.isnan(AngleDiff):\n",
    "        continue\n",
    "      \n",
    "      SumDistDiff += DistDiff\n",
    "      SumAngleDiff += AngleDiff\n",
    "      TotalEvents += 1\n",
    "      DistDiffList.append(DistDiff)\n",
    "      AngleDiffList.append(AngleDiff)\n",
    "\n",
    "\n",
    "      # Some debugging\n",
    "      if Batch == 0 and e < 5:\n",
    "        EventID = e + Batch*BatchSize + NTrainingBatches*BatchSize\n",
    "        print(\"\\nEvent {}:\".format(EventID))\n",
    "        DataSets[EventID].print()\n",
    "\n",
    "        print(\"Positions: {} vs {} -> {} cm difference\".format(oPos, rPos, DistDiff))\n",
    "        print(\"Directions: {} vs {} -> {} degree difference\".format(oDir, rDir, AngleDiff))\n",
    "\n",
    "  if TotalEvents > 0:\n",
    "    if SumDistDiff / TotalEvents < BestLocation and SumAngleDiff / TotalEvents < BestAngle:\n",
    "      BestLocation = SumDistDiff / TotalEvents\n",
    "      BestAngle = SumAngleDiff / TotalEvents\n",
    "      Improvement = True\n",
    "\n",
    "    print(\"Status: distance difference = {:-6.2f} cm, angle difference = {:-6.2f} deg\".format(SumDistDiff / TotalEvents, SumAngleDiff / TotalEvents))\n",
    "\n",
    "    # I ADDED THIS\n",
    "    plt.scatter(DistDiffList, AngleDiffList, marker='o')\n",
    "    plt.xlabel('Distance Difference')\n",
    "    plt.ylabel('Angle Difference')\n",
    "    plot_filename = \"DistanceVersusAnglePlot\"\n",
    "    #plot_filepath = os.path.join(OutputDirectory, plot_filename+\".png\")\n",
    "    plt.savefig(\"Results\" + \"/\" + plot_filename+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "  return Improvement, SumDistDiff / TotalEvents, SumAngleDiff / TotalEvents\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Main training and evaluation loop\n",
    "\n",
    "TimeConverting = 0.0\n",
    "TimeTraining = 0.0\n",
    "TimeTesting = 0.0\n",
    "\n",
    "Iteration = 0\n",
    "MaxIterations = 50000\n",
    "TimesNoImprovement = 0\n",
    "MaxTimesNoImprovement = 50\n",
    "\n",
    "performance_filename = \"Performance\"\n",
    "#performance_filepath = os.path.join(OutputDirectory, performance_filename+\".txt\")\n",
    "\n",
    "\n",
    "while Iteration < MaxIterations:\n",
    "  stime = time.time()\n",
    "  Iteration += 1\n",
    "  print(\"\\n\\nStarting iteration {}\".format(Iteration))\n",
    "\n",
    "  # Step 1: Loop over all training batches\n",
    "  for Batch in range(0, NTrainingBatches):\n",
    "    print(\"Batch {} / {}\".format(Batch+1, NTrainingBatches))\n",
    "    \n",
    "    # Step 1.1: Convert the data set into the input and output tensor\n",
    "    TimerConverting = time.time()\n",
    "\n",
    "    InputTensor = np.zeros(shape=(BatchSize, XBins, YBins, ZBins, 1))\n",
    "    OutputTensor = np.zeros(shape=(BatchSize, OutputDataSpaceSize))\n",
    "\n",
    "    # Loop over all training data sets and add them to the tensor\n",
    "    for g in range(0, BatchSize):\n",
    "      Event = TrainingDataSets[g + Batch*BatchSize]\n",
    "\n",
    "      # Set all the hit locations and energies\n",
    "      for h in range(0, len(Event.X)):\n",
    "        XBin = int( (Event.X[h] - XMin) / ((XMax - XMin) / XBins) )\n",
    "        YBin = int( (Event.Y[h] - YMin) / ((YMax - YMin) / YBins) )\n",
    "        ZBin = int( (Event.Z[h] - ZMin) / ((ZMax - ZMin) / ZBins) )\n",
    "        if XBin >= 0 and YBin >= 0 and ZBin >= 0 and XBin < XBins and YBin < YBins and ZBin < ZBins:\n",
    "          InputTensor[g][XBin][YBin][ZBin][0] = Event.E[h]\n",
    "\n",
    "      OutputTensor[g][0] = Event.TrackStartX\n",
    "      OutputTensor[g][1] = Event.TrackStartY\n",
    "      OutputTensor[g][2] = Event.TrackStartZ\n",
    "\n",
    "      OutputTensor[g][3] = Event.TrackDirectionX\n",
    "      OutputTensor[g][4] = Event.TrackDirectionY\n",
    "      OutputTensor[g][5] = Event.TrackDirectionZ\n",
    "\n",
    "    TimeConverting += time.time() - TimerConverting\n",
    "\n",
    "\n",
    "\n",
    "    # Step 1.2: Perform the actual training\n",
    "    TimerTraining = time.time()\n",
    "    History = Model.fit(InputTensor, OutputTensor, validation_split=0.1)\n",
    "    Loss = History.history['loss'][-1]\n",
    "    TimeTraining += time.time() - TimerTraining\n",
    "\n",
    "    if Interrupted == True: break\n",
    "\n",
    "  # End for all batches\n",
    "\n",
    "  # Step 2: Check current performance\n",
    "  TimerTesting = time.time()\n",
    "  print(\"\\nCurrent loss: {}\".format(Loss))\n",
    "  Improvement, Dist_Diff, Angle_Diff = CheckPerformance()\n",
    "\n",
    "  if Improvement == True:\n",
    "    TimesNoImprovement = 0\n",
    "\n",
    "    print(\"\\nFound new best model and performance!\")\n",
    "  else:\n",
    "    TimesNoImprovement += 1\n",
    "\n",
    "  TimeTesting += time.time() - TimerTesting\n",
    "\n",
    "  # Exit strategy\n",
    "  if TimesNoImprovement == MaxTimesNoImprovement:\n",
    "    print(\"\\nNo improvement for {} iterations. Quitting!\".format(MaxTimesNoImprovement))\n",
    "    break;\n",
    "\n",
    "  print(\"\\n\\nTotal time converting per Iteration: {} sec\".format(TimeConverting/Iteration))\n",
    "  print(\"Total time training per Iteration:   {} sec\".format(TimeTraining/Iteration))\n",
    "  print(\"Total time testing per Iteration:    {} sec\".format(TimeTesting/Iteration))\n",
    "\n",
    "  performance_file = open(\"Results/alpha_reversed.txt\", \"a\")\n",
    "  time_taken = time.time() - stime\n",
    "  tofile = \"\\nCurrent loss: {}\".format(Loss) + \"\\nDistance Difference: {}\".format(Dist_Diff) + \"\\nAngle Difference: {}\".format(Angle_Diff) + \"\\nTime Taken: {}\".format(time_taken) + \"\\n\"\n",
    "  performance_file.write(tofile)\n",
    "  performance_file.close()\n",
    "\n",
    "  if Improvement == True:\n",
    "    model_path = os.path.join(OutputDirectory, 'saved_model_{}.h5'.format(Iteration))\n",
    "    Model.save(filepath=model_path)\n",
    "\n",
    "  # Take care of Ctrl-C\n",
    "  if Interrupted == True: break\n",
    "\n",
    "\n",
    "# End: for all iterations\n",
    "\n",
    "#input(\"Press [enter] to EXIT\")\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLunBpzZmbAk"
   },
   "outputs": [],
   "source": [
    "!cat '/content/Results/Performance.txt'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "URAP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
